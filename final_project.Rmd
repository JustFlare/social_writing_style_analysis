---
title: "Social Writing Style Analysis"
author: "Moiseev G., Nasedkin A."
output:
  html_document:
    toc: yes
    toc_float: yes
---

## Introduction

Project description, data and scripts for comments retrieving from VK groups can be found in our [GitHub repository](https://github.com/JustFlare/social_writing_style_analysis)

Prerequisites:
* the *csv_data.csv* file should be located in the same directory with current .Rmd file
* this file can be found [here](https://github.com/JustFlare/social_writing_style_analysis/blob/master/final_project.Rmd)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(lme4)
library(car)
library(hash)
library(strucchange)

rm(list=ls())
```

## Data

### Load the data. Gather desired columns. Filter the data by .95 quantile of target variable.
```{r}
df <- read.csv(file="csv_data.csv", header=TRUE, sep=",")
df <- df[c("u_id", "u_sex", "u_year", "u_uni", "f_char_cnt_sent", "f_word_cnt_sent", "f_word_len_avg", "f_punct_cnt_word")]
df <- df[df$f_punct_cnt_word < quantile(df$f_punct_cnt_word, 0.95),]
head(df, 10)
```

### Compute mean feature values by user
```{r}
avg_df <- aggregate(. ~ u_id + u_sex + u_year + u_uni, df, mean)
avg_df[avg_df$u_id == 242515614, ]
```

### General statistics
```{r}
sprintf('Distinct users: %d', length(unique(df$u_id)))
sprintf('Distinct comments: %d', nrow(df))
sprintf('Average number of comments per user: %f', nrow(df) / length(unique(df$u_id)))
```

### Distributions
```{r warning=FALSE}
# users distribution by comments count
stat <- df %>%
  group_by(u_id) %>%
  summarise(cnt_comment = n()) %>%
  group_by(cnt_comment) %>%
  summarise(cnt_user = n())

stat %>%
  ggplot(aes(cnt_comment, cnt_user)) +
  geom_line() +
  # Right boundary set in order to zoom chart a little
  # cause there are some users with astonishing count of comments
  # Left boundary set due to the assumption in data
  scale_x_continuous(limits = c(10, 100)) +
  labs(
    title="Distribution of users by number of their comments",
    x="Number of comments",
    y="Number of unique users"
  )
```
```{r}
# comments count by birth year of their author
df %>%
  group_by(u_year) %>%
  summarise(cnt_comment = n()) %>%
  ggplot(aes(u_year, cnt_comment)) +
  geom_line() +
  scale_x_discrete(limits = seq(min(df$u_year), max(df$u_year), 3)) +
  theme(axis.text.x=element_text(angle=30)) +
  labs(
    title="Distribution of comments by their author age",
    x="Author age",
    y="Number of comments"
  )
```
```{r}
# birth year distribution
unique(df[c('u_id', 'u_year')]) %>%
  group_by(u_year) %>%
  summarise(cnt_user = n()) %>%
  ggplot(aes(u_year, cnt_user)) +
  geom_line() +
  scale_x_discrete(limits = seq(min(df$u_year), max(df$u_year), 3)) +
  theme(axis.text.x=element_text(angle=30)) +
  labs(
    title="Distribution of users by their age",
    x="Age",
    y="Number of unique users"
  )
```

## Regression

```{r constants}
pvalue_threshold = 0.05
awl_threshold = 25
```


### Fit and store the linear regression models by each user who has enough comments
```{r}

# function to return the p-value from fitted regression
lmp <- function (modelobject) {
  if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
  
  out <- tryCatch(
    {
      f <- summary(modelobject)$fstatistic
      p <- pf(f[1],f[2],f[3],lower.tail=F)
      attributes(p) <- NULL
      return(p)   
    },
    error=function(err) {
      return(NaN)
    }
  )
}

# leave only users with more than 30 comments
user_count <- table(df$u_id)
df_text_based <- subset(df, u_id %in% names(user_count[user_count > 30]))
df_text_based <- df_text_based[c("u_id", "f_char_cnt_sent", "f_word_cnt_sent", "f_word_len_avg", "f_punct_cnt_word")]

users <- sort(unique(df_text_based$u_id))
user_lm <- hash()

for (id in users) {
  temp_lm <- lm(f_punct_cnt_word ~ ., data = df_text_based[df_text_based$u_id == id, names(df_text_based) != "u_id"])
  temp_lmp <- lmp(temp_lm)
  if (is.nan(temp_lmp)) {
    next
  }
  if (temp_lmp < pvalue_threshold) {
    user_lm[id] <- temp_lm
  }
}

print("percent of significant regressions: ")
length(user_lm) / length(users)

clear(user_lm)
rm(id)
```

We can see that most regressions are statistically significant. It means that for most users factorial variance is bigger than residual. Consequently, we can say that there is a linear dependancy between our text-based factors and number of punctuations per word. But more than 40% of regressions are insignificant which probably means that text-based set of regressors doesn't have enough descriptive power.

### Fit the linear regression model on the whole mean data

Lets include personal features such as sex and birth year. To make such regression we use aggregated text-based features (simple arithmetic mean).

```{r}
# remain sex, year and comment features
avg_df_year <- avg_df[c("u_sex", "u_year", "f_char_cnt_sent", "f_word_cnt_sent", "f_word_len_avg", "f_punct_cnt_word")]
avg_fit_year <- lm(f_punct_cnt_word ~ ., data = avg_df_year)
summary(avg_fit_year)


```
We can see that all absolute t-values for our factors are bigger than 1.96 which means that they all are statistically significant (we can't replace them with null). So, we can say that personal features are at least useful in such model.

The p-value is lower than 0.05 => the regression is significant. But its' value is extremely small which is a bit suspicious. The reason for that can be noise in our data or just its' large amount. Maybe visualization of this model would make things more clear?

```{r}
plot(avg_fit_year)
```

The first thing that catches the eye is some outliers and noises in the data. After some analysis we figure out that this is connected with very high average word length. So, we filtered out all word below {awl_threshold}

```{r}
# clean some data with very long average word length
avg_df_year <- avg_df_year[avg_df_year$f_word_len_avg < awl_threshold,]
avg_fit_year <- lm(f_punct_cnt_word ~ ., data = avg_df_year)
summary(avg_fit_year)
plot(avg_fit_year)
```

But even after filtering, graphs reveal some notable patterns. It seems like our model is not the best one for fitting such data, as it may contain non-linear dependencies (on the first graph the red curve is not quite flat). But it is certainly not the worst case for linear model and it is possible to use it here.
On the second graph (Normal Q-Q) there are still some residuals with big deviations (on the left side), but the main quantity is distributed normally. Following the third one (Scale-Location) we may presume the lack of data homoscedasticity. Finally, the last graph shows outliers which should be further deeply analysed.


Also, we want to evaluate the influence of year in our model. Firts thing to see - how model would work without it:

### Do it again but without year
```{r}
# remain sex, year and comment features
avg_df_wo_year <- avg_df[c("u_sex", "f_char_cnt_sent", "f_word_cnt_sent", "f_word_len_avg", "f_punct_cnt_word")]
avg_df_wo_year <- avg_df_wo_year[avg_df_wo_year$f_word_len_avg < awl_threshold ,]

avg_fit_wo_year <- lm(f_punct_cnt_word ~ ., data = avg_df_wo_year)
summary(avg_fit_wo_year)
```

Again we see extremely small p-value (due to the data) but now the F-statistics is smaller. 
Moreover, we can make sense from another measure - Adjusted R-squared which is useful in model comparison. Adjusted R-squared as simple R-squared indicates how well terms fit a line but it also penalizes for useless variables. We can see that without year Adjusted R-squared is lower => the model withour year is making more errors or its' regressors aren't efficient enough.
So, we can state that number of punctuation marks depends on birth year somehow. In other case Adjusted R-squared would penalize us for useless variable and the model without year would have greater Adjusted R-squared value.


## check the linear dependancy between punctuation number and birth year

To exlucde influnce of other factors in the model we need to analyze residuals of the without-year model for each birth year group. To achieve this we aggregate the data by year and fit regression on it.
```{r}
df_t <- df[c("u_year", "f_char_cnt_sent", "f_word_cnt_sent", "f_word_len_avg", "f_punct_cnt_word")]
avg_by_year_df <- aggregate(. ~ u_year, df_t, mean)

avg_by_year <- lm(f_punct_cnt_word ~ ., data = avg_by_year_df)
plot(avg_by_year)

```

From the first and the third plots we again may state that data is spread non-linearly. Now, unlike the first regression, it can be seen more clearly. And again from the second plot we can see that data is distributed quite normally. From the fourth plot we can notice some outliers on the right side of the plot.

```{r}
avg_by_year_df %>%
ggplot(aes(u_year, f_punct_cnt_word)) +
geom_smooth(method = "lm") +
geom_point() +
scale_x_discrete(limits = seq(min(avg_by_year_df$u_year), max(avg_by_year_df$u_year), 3)) +
theme(axis.text.x=element_text(angle=30)) +
labs(
title="LM smoothing",
x="Birth year",
y="Avg punct by word"
)
```

From this plot we can clearly see that dependency between number of punctuation marks and birth year is non-linear. Also we see that older people tends to use more punctuation marks than younger ones.
It is worth saying that in the last model the influence of older people on common trend in unpropotionally high that in regressions built on authors.

